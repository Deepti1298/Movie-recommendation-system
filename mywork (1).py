# -*- coding: utf-8 -*-
"""mywork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tq_qERJR1MmP9USF7niADZ_sHh1vn59Y
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import ast
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Load the dataset
df_movies = pd.read_csv('tmdb_5000_movies.csv', encoding='utf-8')
df_credits=pd.read_csv('tmdb_5000_credits.csv')

df_movies.head()

df_credits.head()

df=df_movies.merge(df_credits,on='title')

df.head()

# df.to_csv('movies.csv', index=False, encoding='utf-8')

df = pd.read_csv('movies.csv')
df.head(2)

"""## Exploratory Data Analysis"""

# Info of the dataset
df.info()

# Summary of the numerical attributes
df.describe()

df.isnull().sum()

# Drop na values
df.dropna(inplace=True)

# Check duplicated values
df.duplicated().sum()

df.isna().sum()

# # Create a histogram for each numerical column in the DataFrame.

# Specify columns for which you want to create histograms
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Create histograms for selected numerical columns
df[numerical_columns].hist(bins=20, figsize=(20, 15), color='skyblue', edgecolor='black', grid=False, layout=(3, 3), sharex=False)
plt.suptitle('Histograms of Numerical Features', fontsize=20, y=0.95)
plt.show()

# Convert 'release_date' to datetime
df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')

# Extract year from 'release_date'
df['release_year'] = df['release_date'].dt.year

# Plotting

# Number of movies released per year
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='release_year')
plt.xticks(rotation=90)
plt.title('Number of Movies Released Per Year')
plt.ylabel('Number of Movies')
plt.xlabel('Year')
plt.tight_layout()
plt.show()

# Budget vs Revenue
plt.figure(figsize=(10, 6))
plt.scatter(df['budget'], df['revenue'])
plt.title('Budget vs Revenue')
plt.xlabel('Budget')
plt.ylabel('Revenue')
plt.tight_layout()
plt.show()

# Selecting numerical columns for correlation analysis
numerical_cols = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']

# Calculating correlation matrix
corr_matrix = df[numerical_cols].corr()

# Plotting the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""* From the correlation matrix, we can identify the following insights regarding relevant information:

1.  Revenue and Budget: There's a significant positive correlation between revenue and budget, indicating that movies with higher budgets tend to generate higher revenues.

2.   Vote Count and Popularity: There's a strong positive correlation between vote count and popularity, suggesting that more popular movies receive more votes.

3.    Revenue and Vote Count: Revenue also shows a positive correlation with vote count, which could imply that movies with higher revenues tend to have more votes, possibly due to higher visibility or popularity.

4.   Revenue and Popularity: There's a positive correlation between revenue and popularity, indicating that more popular movies tend to generate higher revenues.

## Feature Engineering
"""

# Define the function to convert text to list
def convert(text):
    l = []
    try:
        for i in ast.literal_eval(text):
            l.append(i['name'])
    except:
        l = []
    return l

# Apply the convert function to 'genres', 'keywords', 'production_companies', and 'production_countries'
df['genres'] = df['genres'].apply(convert)
df['keywords'] = df['keywords'].apply(convert)
df['production_companies'] = df['production_companies'].apply(convert)
df['production_countries'] = df['production_countries'].apply(convert)

print('Feature engineering applied to genres, keywords, production_companies, and production_countries.')

df.head()

import ast
def convert_cast(text):
  l=[]
  counter=0
  for i in ast.literal_eval(text):
    if counter<3:
     l.append(i['name'])
    counter=1

  return l

df['cast']=df['cast'].apply (convert)

import ast
def fetch_Director(text):
  l=[]
  for i in ast.literal_eval(text):
    if i ['job']=='Director':
      l.append(i['name'])
      break

  return l

df['crew']=df['crew'].apply(fetch_Director)

# Apply the lambda function to 'overview' column to split the text into words
df['overview'] = df['overview'].apply(lambda x: x.split() if isinstance(x, str) else x)

# Display the first few rows to verify the changes
print(df[['original_title', 'overview']].head())

def remove_space(word):
  l=[]
  for i in word:
    l.append(i.replace(" ",""))

  return l

df['cast']=df['cast'].apply(remove_space)
df['crew']=df['crew'].apply(remove_space)
df['keywords']=df['keywords'].apply(remove_space)
df['genres']=df['genres'].apply(remove_space)

df['tags']=df['overview']+df['genres']+df['keywords']+df['cast']+df['crew']

df.head(3)

# Drop the original yr columns used to create 'tags'
df.drop(columns=['overview', 'genres', 'keywords', 'cast', 'crew'], inplace=True)

df.shape

new_df=df[['movie_id','title','tags']]

new_df.head()

# Convert lists in 'tags' column to strings
new_df['tags'] = new_df['tags'].apply(lambda x: ' '.join(map(str, x)))

new_df['tags']

"""## Model Training

Create a CountVectorizer instance with a vocabulary limited to the top 5000 most frequent words and excludes common English stop words during the tokenization process.
"""

# Importing the library

from sklearn.feature_extraction.text import CountVectorizer

def create_text_vectorizer(max_features=5000, stop_words='english'):
    """
    Create a text vectorizer using CountVectorizer.

    Parameters:
    - max_features (int, optional): Maximum number of features (words) to be included in the vocabulary.
      Defaults to 5000.
    - stop_words (str or list, optional): Specifies whether to remove common English stop words during tokenization.
      Defaults to 'english', which removes common English stop words. Pass None or an empty list to keep all words.

    Returns:
    - CountVectorizer: Instance of the CountVectorizer class configured with the specified parameters.
    """
    return CountVectorizer(max_features=max_features, stop_words=stop_words)

# Create a text vectorizer instance
cv = create_text_vectorizer()

# Transform the 'tags' column in the DataFrame into a sparse matrix of word counts
vector = cv.fit_transform(new_df['tags']).toarray()

from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_similarity(matrix):
    """
    Calculate the cosine similarity between rows of a matrix.

    Parameters:
    - matrix (numpy.ndarray or scipy.sparse matrix): Input matrix where each row represents a data point.

    Returns:
    - numpy.ndarray: Cosine similarity matrix, where element (i, j) represents the cosine similarity between rows i and j.
    """
    return cosine_similarity(matrix)

# Calculate cosine similarity matrix
similarity = calculate_cosine_similarity(vector)

new_df[new_df['title'] == 'The Lego Movie'].index[0]

new_df.head(2)

def recommend(title):
    try:
        # Convert input title to lowercase for case-insensitive matching
        title_lower = title.lower()

        # Find the index of the movie with the matching title (case-insensitive)
        indx = new_df[new_df['title'].str.lower() == title_lower].index[0]
        indx = new_df.index.get_loc(indx)

        # Compute movie recommendations based on similarity
        distances = sorted(list(enumerate(similarity[indx])), key=lambda x: x[1], reverse=True)[1:5]

        movies = []
        for i in distances:
            # Append recommended movies to the list
            movies.append(new_df.iloc[i[0]].title)

        # Include the input movie in the recommendations
        movies.insert(0, new_df.iloc[indx].title)

        return movies
    except IndexError:
        # Handle the case where no movies are found
        return ["No movies recommended."]

recommend('The lego movie')

